{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole_dqn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3S2iQB7H9tPWu2LsAz4qi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O_FA9TFmMRM6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gym\n",
        "import numpy as np\n",
        "import random as rand"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "  def __init__(self):\n",
        "    self.env = gym.make('CartPole-v1')\n",
        "    self.state_size = self.env.observation_space.shape[0]\n",
        "    self.action_size = self.env.action_space.n\n",
        "\n",
        "    self.node_num = 12\n",
        "    self.learning_rate = 0.001\n",
        "    self.epochs_cnt = 5\n",
        "    self.model = self.build_model()\n",
        "\n",
        "    self.discount_rate = 0.97\n",
        "    self.penalty = -100\n",
        "\n",
        "    self.episode_num = 500\n",
        "\n",
        "    self.replay_memory_limit = 2048\n",
        "    self.replay_size = 32\n",
        "    self.replay_memory = []\n",
        "\n",
        "    self.epsilon = 0.99\n",
        "    self.epsilon_decay = 0.2 # 0.2 episode의 20%가 되면 수행, epsilon이 0이 된다.\n",
        "    self.epsilon_min = 0.05\n",
        "\n",
        "    self.moving_avg_size = 20\n",
        "    self.reward_list = []\n",
        "    self.count_list = []\n",
        "    self.moving_avg_list = []\n",
        "\n",
        "  def build_model(self):\n",
        "    input_states = Input(shape=(1,self.state_size), name='input_states')\n",
        "    x = (input_states)\n",
        "    x = Dense(self.node_num, activation='relu')(x)\n",
        "    out_actions = Dense(self.action_size, activation='linear', name='output')(x)\n",
        "    model = tf.keras.models.Model(inputs=[input_states], outputs=[out_actions])\n",
        "    model.compile(optimizer=Adam(lr=self.learning_rate),\n",
        "                  loss='mean_squared_error'\n",
        "                  )\n",
        "    model.summary()\n",
        "    return model\n",
        "  \n",
        "  def train(self):\n",
        "    for episode in range(self.episode_num):\n",
        "      state = self.env.reset()\n",
        "      Q, count, reward_tot = self.take_action_and_append_memory(episode, state)\n",
        "\n",
        "      if count < 500:\n",
        "        reward_tot = reward_tot - self.penalty\n",
        "\n",
        "      self.reward_list.append(reward_tot)\n",
        "      self.count_list.append(count)\n",
        "      self.moving_avg_list.append(self.moving_avg(self.count_list, self.moving_avg_size))\n",
        "\n",
        "      self.train_mini_batch(Q)\n",
        "\n",
        "      if (episode % 10 == 0):\n",
        "        print(\"episode:{}, moving_avg:{}, rewards_avg:{}\".format(episode, self.moving_avg_list[-1], np.mean(self.reward_list)))"
      ],
      "metadata": {
        "id": "PmPNoo8HPF5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}